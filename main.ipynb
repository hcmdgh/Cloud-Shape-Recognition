{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 数据预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 载入图片数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import Tensor \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torchvision import transforms\n",
    "from PIL import Image \n",
    "import csv \n",
    "import numpy as np \n",
    "from numpy import ndarray \n",
    "import math \n",
    "from typing import Any, Optional, Union, Callable\n",
    "from tqdm.auto import tqdm \n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_dataset() -> ndarray:\n",
    "    sample_list = [] \n",
    "    \n",
    "    with open('./data/train.csv', 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.DictReader(fp)\n",
    "        \n",
    "        for row in reader:\n",
    "            filename = row['FileName']\n",
    "            path = f'./data/image/{filename}'\n",
    "            label = int(row['Code']) - 1\n",
    "            \n",
    "            sample = (path, label)\n",
    "            sample_list.append(sample)\n",
    "\n",
    "    sample_arr = np.array(sample_list, dtype=object)\n",
    "    \n",
    "    return sample_arr \n",
    "\n",
    "\n",
    "def load_test_dataset() -> ndarray:\n",
    "    sample_list = [] \n",
    "    \n",
    "    with open('./data/test.csv', 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.DictReader(fp)\n",
    "        \n",
    "        for row in reader:\n",
    "            filename = row['FileName']\n",
    "            path = f'./data/image/{filename}'\n",
    "            label = -1 \n",
    "            \n",
    "            sample = (path, label)\n",
    "            sample_list.append(sample)\n",
    "\n",
    "    sample_arr = np.array(sample_list, dtype=object)\n",
    "    \n",
    "    return sample_arr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['./data/image/fdad0045a3c845f89a3894975a1e9a87.jpg', 9],\n",
       "       ['./data/image/6e3f9856d6644c3fa6513734bb5c6145.png', 18],\n",
       "       ['./data/image/ac81ffdd29924e8f89abac01481b90b7.jpg', 17],\n",
       "       ...,\n",
       "       ['./data/image/422e818c7ee24682be4713455616d2b5.jpg', 24],\n",
       "       ['./data/image/36f95c45ce0d41f3b01a6bfabab8ec4c.jpg', 24],\n",
       "       ['./data/image/36e3fe8bf6d44117a762946376ba695f.jpg', 26]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_train_dataset()\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8167"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['./data/image/85a2b5280cf44ee0bbedbc97ee45d2d5.jpg', -1],\n",
       "       ['./data/image/f75f1e81924a43f2b6491035b6c7a491.jpg', -1],\n",
       "       ['./data/image/aad9b2f79a9f46d4a343c3cface61871.jpg', -1],\n",
       "       ...,\n",
       "       ['./data/image/66c65624546f48d294da7618508c7831.jpg', -1],\n",
       "       ['./data/image/54139ab75f1648fa935a6d5233ebf1fd.jpg', -1],\n",
       "       ['./data/image/8db960a2c26f4a81ae697c40f4281892.jpg', -1]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = load_test_dataset()\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2043"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6533, 2), (1634, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_train_val_set(arr: np.ndarray, \n",
    "                        train_ratio: float,\n",
    "                        val_ratio: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "    assert math.isclose(train_ratio + val_ratio, 1.) \n",
    "    N = len(arr) \n",
    "    perm = np.random.permutation(N)\n",
    "    \n",
    "    train_cnt = int(N * train_ratio)\n",
    "    \n",
    "    train_set = arr[perm[:train_cnt]]\n",
    "    val_set = arr[perm[train_cnt:]]\n",
    "    assert len(train_set) + len(val_set) == N  \n",
    "\n",
    "    return train_set, val_set \n",
    "\n",
    "\n",
    "train_dataset, val_dataset = split_train_val_set(\n",
    "    arr = train_dataset, \n",
    "    train_ratio = 0.8, \n",
    "    val_ratio = 0.2, \n",
    ")\n",
    "\n",
    "train_dataset.shape, val_dataset.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 载入DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize([128, 128]),\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 sample_arr: np.ndarray):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sample_arr = sample_arr \n",
    "        \n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, int]:\n",
    "        path, label = self.sample_arr[index]\n",
    "        \n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        img = transform(img)\n",
    "        \n",
    "        return img, label \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sample_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 26, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset = ImageDataset(train_dataset),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    drop_last = False, \n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset = ImageDataset(val_dataset),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    drop_last = False, \n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset = ImageDataset(test_dataset),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    drop_last = False, \n",
    ")\n",
    "\n",
    "len(train_dataloader), len(val_dataloader), len(test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_finetune import make_model\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = make_model(\n",
    "            'xception', \n",
    "            num_classes = 2, \n",
    "            pretrained = True,\n",
    "            input_size = (224, 224),\n",
    "        )\n",
    "        \n",
    "        self.model._features[0] = nn.Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "\n",
    "        self.model = self.model._features[0:16]\n",
    "\n",
    "    def forward(self, \n",
    "                batch: Tensor) -> Tensor:\n",
    "        return self.model(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleConv(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channel: int, \n",
    "                 out_channel: int, \n",
    "                 norm: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channel = in_channel \n",
    "        self.out_channel = out_channel \n",
    "\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=1),\n",
    "            norm,\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                batch: Tensor) -> Tensor:\n",
    "        batch_size, in_channel, height, width = batch.shape \n",
    "        assert in_channel == self.in_channel \n",
    "                \n",
    "        out = self.conv(batch)\n",
    "        assert out.shape == (batch_size, self.out_channel, height, width)\n",
    "                \n",
    "        return out \n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channel: int, \n",
    "                 out_channel: int, \n",
    "                 norm: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channel = in_channel \n",
    "        self.out_channel = out_channel \n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            SingleConv(in_channel, out_channel, norm),\n",
    "            SingleConv(out_channel, out_channel, norm),\n",
    "        ) \n",
    "\n",
    "    def forward(self, \n",
    "                batch: Tensor) -> Tensor:\n",
    "        batch_size, in_channel, height, width = batch.shape \n",
    "        assert in_channel == self.in_channel \n",
    "                \n",
    "        out = self.conv(batch)\n",
    "        assert out.shape == (batch_size, self.out_channel, height, width)\n",
    "                \n",
    "        return out \n",
    "    \n",
    "    \n",
    "ConvBlock3 = SingleConv \n",
    "\n",
    "\n",
    "class ConvBlock7(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channel: int, \n",
    "                 out_channel: int, \n",
    "                 norm: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channel = in_channel \n",
    "        self.out_channel = out_channel \n",
    "\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size=7, stride=2, padding=3),\n",
    "            norm,\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                batch: Tensor) -> Tensor:\n",
    "        batch_size, in_channel, height, width = batch.shape \n",
    "        assert in_channel == self.in_channel \n",
    "                \n",
    "        out = self.conv(batch)\n",
    "        assert out.shape == (batch_size, self.out_channel, height // 2, width // 2)\n",
    "                \n",
    "        return out \n",
    "    \n",
    "    \n",
    "class ConvBlock6(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channel: int, \n",
    "                 out_channel: int, \n",
    "                 norm: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channel = in_channel \n",
    "        self.out_channel = out_channel \n",
    "\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size=5, stride=1, padding=2),\n",
    "            norm,\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                batch: Tensor) -> Tensor:\n",
    "        batch_size, in_channel, height, width = batch.shape \n",
    "        assert in_channel == self.in_channel \n",
    "                \n",
    "        out = self.conv(batch)\n",
    "        assert out.shape == (batch_size, self.out_channel, height, width)\n",
    "                \n",
    "        return out \n",
    "\n",
    "\n",
    "ConvBlock5 = ConvBlock6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channel: int, \n",
    "                 out_channel: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv = DoubleConv(in_channel, out_channel)\n",
    "\n",
    "    def forward(self, \n",
    "                batch: Tensor) -> Tensor:\n",
    "        \n",
    "        batch_size, in_channel, height, width = batch.shape \n",
    "        assert in_channel == self.in_channel \n",
    "        \n",
    "        h = self.upsample(batch)\n",
    "        assert h.shape == (batch_size, in_channel, height * 2, width * 2)\n",
    "                \n",
    "        out = self.conv(h)\n",
    "        assert out.shape == (batch_size, self.out_channel, height * 2, width * 2)\n",
    "\n",
    "        return out "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat \n",
    "from self_attention_cv import TransformerEncoder\n",
    "\n",
    "\n",
    "def expand_to_batch(tensor, desired_size):\n",
    "    tile = desired_size // tensor.shape[0]\n",
    "    return repeat(tensor, 'b ... -> (b tile) ...', tile=tile)\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 img_dim,\n",
    "                 in_channels = 3,\n",
    "                 patch_dim = 16,\n",
    "                 num_classes = 2,\n",
    "                 dim = 512,\n",
    "                 blocks = 6,\n",
    "                 heads = 4,\n",
    "                 dim_linear_block = 1024,\n",
    "                 dim_head = None,\n",
    "                 dropout = 0, \n",
    "                 transformer = None, \n",
    "                 classification = True):\n",
    "        \"\"\"\n",
    "        Minimal re-implementation of ViT\n",
    "        Args:\n",
    "            img_dim: the spatial image size\n",
    "            in_channels: number of img channels\n",
    "            patch_dim: desired patch dim\n",
    "            num_classes: classification task classes\n",
    "            dim: the linear layer's dim to project the patches for MHSA\n",
    "            blocks: number of transformer blocks\n",
    "            heads: number of heads\n",
    "            dim_linear_block: inner dim of the transformer linear block\n",
    "            dim_head: dim head in case you want to define it. defaults to dim/heads\n",
    "            dropout: for pos emb and transformer\n",
    "            transformer: in case you want to provide another transformer implementation\n",
    "            classification: creates an extra CLS token that we will index in the final classification layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible by img dim {img_dim}'\n",
    "        self.p = patch_dim\n",
    "        self.classification = classification\n",
    "\n",
    "        # tokens = number of patches\n",
    "        tokens = (img_dim // patch_dim) ** 2\n",
    "\n",
    "        self.token_dim = in_channels * (patch_dim ** 2)\n",
    "        self.dim = dim\n",
    "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "\n",
    "        # Projection and pos embeddings\n",
    "        self.project_patches = nn.Linear(self.token_dim, dim)\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "        if transformer is None:\n",
    "            self.transformer = TransformerEncoder(dim, blocks=blocks, heads=heads,\n",
    "                                                  dim_head=self.dim_head,\n",
    "                                                  dim_linear_block=dim_linear_block,\n",
    "                                                  dropout=dropout)\n",
    "        else:\n",
    "            self.transformer = transformer\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        # Create patches\n",
    "        # from [batch, channels, h, w] to [batch, tokens , N], N=p*p*c , tokens = h/p *w/p\n",
    "        img_patches = rearrange(img,\n",
    "                                'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.p, patch_y=self.p)\n",
    "\n",
    "        batch_size, tokens, _ = img_patches.shape\n",
    "\n",
    "        # project patches with linear layer + add pos emb\n",
    "        img_patches = self.project_patches(img_patches)\n",
    "\n",
    "        img_patches = torch.cat((expand_to_batch(self.cls_token, desired_size=batch_size), img_patches), dim=1)\n",
    "\n",
    "        # add pos. embeddings. + dropout\n",
    "        # indexing with the current batch's token length to support variable sequences\n",
    "        img_patches = img_patches + self.pos_emb1D[:tokens + 1, :]\n",
    "        patch_embeddings = self.emb_dropout(img_patches)\n",
    "\n",
    "        # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
    "        y = self.transformer(patch_embeddings, mask)\n",
    "\n",
    "        # we index only the cls token for classification. nlp tricks :P\n",
    "        return self.mlp_head(y[:, 0, :]) if self.classification else y[:, 1:, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Utrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utrans(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=1):\n",
    "        super().__init__()\n",
    "\n",
    "        ch = 32\n",
    "        filters = [ch, ch*2, ch*4, ch*8,ch*16,ch*32] #32,64,128,256,512,1024\n",
    "        self.ori_1 = ConvBlock3(in_ch, filters[0]//2)#([1, 32, 128, 128])\n",
    "        self.ori_2 = ConvBlock7(in_ch, filters[0])#([1, 32, 64, 64])\n",
    "\n",
    "        self.chao_in = ConvBlock7(in_ch, filters[0])#([1, 32, 64, 64])\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.adapool= nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv2 = ConvBlock6(filters[1]+32, filters[1])\n",
    "        self.conv3 = ConvBlock5(filters[1], filters[2])\n",
    "        self.conv4 = ConvBlock3(filters[2], filters[2])\n",
    "        self.conv5 = ConvBlock3(filters[2], filters[2])\n",
    "        self.conv6 = ConvBlock3(filters[2], filters[3])\n",
    "\n",
    "        self.conv7 = ConvBlock3(728,1024)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512 * 4 * 4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 2)\n",
    "\n",
    "        self.img_dim_vit = 128 // 16 \n",
    "\n",
    "        self.vit2 = ViT(img_dim=128,  # 8\n",
    "                       in_channels=16,  # encoder channels  #1024\n",
    "                       patch_dim=16,\n",
    "                       dim=256,  # vit out channels for decoding  #1024\n",
    "                       blocks=24 , # 12\n",
    "                       heads=8,  # 4\n",
    "                       dim_linear_block=256,  # 1024\n",
    "                       classification=False)\n",
    "        \n",
    "        self.vit3 = ViT(img_dim=96,  # 8\n",
    "                       in_channels=64,  # encoder channels  #1024\n",
    "                       patch_dim=4,\n",
    "                       dim=256,  # vit out channels for decoding  #1024\n",
    "                       blocks=6 , # 12\n",
    "                       heads=4,  # 4\n",
    "                       dim_linear_block=256,  # 1024\n",
    "                       classification=False)\n",
    "        \n",
    "        self.xception = Xception()\n",
    "            \n",
    "        self.classifier = nn.Sequential(  \n",
    "            nn.Linear(1792, 28),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1,x2,x3):\n",
    "        ori_1= self.ori_1(x1)\n",
    "        # print(ori_1.shape)\n",
    "        ori_2 = self.ori_2(x2)\n",
    "        # print(ori_2.shape)\n",
    "        ori_p1=self.pooling(ori_2)\n",
    "\n",
    "        chao_1=self.chao_in(x2)\n",
    "        # print(\"chao_1\",chao_1.shape)\n",
    "        chao_p1 = self.pooling(chao_1)\n",
    "\n",
    "        chao_2=self.chao_in(x3)\n",
    "        # print(\"chao_21\",chao_2.shape)\n",
    "        chao_p2 = self.pooling(chao_2)\n",
    "\n",
    "        ca1 = torch.cat((ori_p1, chao_p1,chao_p2), dim=1)\n",
    "        conv2=self.conv2(ca1)\n",
    "        p2=self.pooling(conv2)\n",
    "        conv3 = self.conv3(p2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "        p3 = self.pooling(conv5)\n",
    "        conv6 = self.conv6(p3)\n",
    "        \n",
    "        # print(\"conv2\",conv2.shape)\n",
    "        y_2 = self.vit3(conv2) #32\n",
    "        # print(\"y_2\" ,y_2.shape)\n",
    "        y_2= rearrange(y_2, 'b (x y) dim -> b dim x y ', x=self.img_dim_vit, y=self.img_dim_vit)\n",
    "    \n",
    "        ori= self.ori_1(x1)#128\n",
    "        # print(\"oriori\",ori.shape)\n",
    "        y_1 = self.vit2(ori) \n",
    "        y_1= rearrange(y_1, 'b (x y) dim -> b dim x y ', x=self.img_dim_vit, y=self.img_dim_vit)\n",
    "\n",
    "        ca1 = torch.cat([ conv6,y_1,y_2], dim=1)\n",
    "        \n",
    "        p=self.xception(x1)\n",
    "        p =self.conv7(p)\n",
    "        ca1 = torch.cat([p ,  ca1], dim=1)    \n",
    "        ca1 =self.adapool(ca1)\n",
    "        x = torch.flatten(ca1, start_dim=1)\t# 展平后再传入全连接层\n",
    "        x = self.classifier( x)\n",
    "          \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:7'\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.001\n",
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(DEVICE)\n",
    "\n",
    "model = Utrans().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f486d40b49bf42bea46c2529265c36f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5d1890a0214a8190bf9a780b20781b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Step:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 1, Loss: 1.98591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b7ba496d5c4ab2abf894a60dc35a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Step:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch: 1, Val Acc: 0.4137\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, NUM_EPOCHS + 1), desc='Train Epoch'):\n",
    "    model.train()\n",
    "    \n",
    "    loss_list: list[float] = []\n",
    "    \n",
    "    for step, (img_batch, label_batch) in enumerate(tqdm(train_dataloader, desc='Train Step'), start=1):\n",
    "        img_batch = img_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        pred_batch = model(img_batch, img_batch, img_batch)\n",
    "\n",
    "        loss = F.cross_entropy(input=pred_batch, target=label_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_list.append(float(loss))\n",
    "        \n",
    "    print(f\"[Train] Epoch: {epoch}, Loss: {np.mean(loss_list):.5f}\")\n",
    "        \n",
    "    model.eval() \n",
    "    \n",
    "    pred_list: list[ndarray] = []\n",
    "    label_list: list[ndarray] = []\n",
    "        \n",
    "    for step, (img_batch, label_batch) in enumerate(tqdm(val_dataloader, desc='Val Step'), start=1):\n",
    "        img_batch = img_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_batch = model(img_batch, img_batch, img_batch)\n",
    "        \n",
    "        pred_list.append(pred_batch.detach().cpu().numpy())\n",
    "        label_list.append(label_batch.detach().cpu().numpy())\n",
    "\n",
    "    pred_full = np.concatenate(pred_list, axis=0)\n",
    "    label_full = np.concatenate(label_list, axis=0)\n",
    "    pred_full = np.argmax(pred_full, axis=-1)\n",
    "    \n",
    "    val_acc = (pred_full == label_full).mean() \n",
    "    \n",
    "    print(f\"[Val] Epoch: {epoch}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f'./checkpoint/model_state_epoch_{epoch}.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171d294eab744461bf90bc296bc93fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Step:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 9, 17, 24, ..., 17, 26, 24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_EPOCH = 1 \n",
    "\n",
    "model.load_state_dict(torch.load(f'./checkpoint/model_state_epoch_{epoch}.pt'))\n",
    "\n",
    "model.eval() \n",
    "    \n",
    "pred_list: list[ndarray] = []\n",
    "    \n",
    "for step, (img_batch, label_batch) in enumerate(tqdm(test_dataloader, desc='Test Step'), start=1):\n",
    "    img_batch = img_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_batch = model(img_batch, img_batch, img_batch)\n",
    "    \n",
    "    pred_list.append(pred_batch.detach().cpu().numpy())\n",
    "\n",
    "pred_full = np.concatenate(pred_list, axis=0)\n",
    "pred_full = np.argmax(pred_full, axis=-1)\n",
    "pred_full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename_list: list[str] = [] \n",
    "\n",
    "with open('./data/test.csv', 'r', encoding='utf-8') as fp:\n",
    "    reader = csv.DictReader(fp)\n",
    "    \n",
    "    for row in reader:\n",
    "        filename = row['FileName']\n",
    "        test_filename_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred_full) == len(test_filename_list)\n",
    "\n",
    "with open('./data/submission.csv', 'w', encoding='utf-8') as fp:\n",
    "    writer = csv.DictWriter(fp, fieldnames=['FileName', 'Code'])\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for pred, filename in zip(pred_full, test_filename_list):\n",
    "        writer.writerow(\n",
    "            dict(\n",
    "                FileName = filename,\n",
    "                Code = pred + 1,  \n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9361eac9592db6036240c1035ef67375b6423b359c6e68ee240849a077bc4c29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
